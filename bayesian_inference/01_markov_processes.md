## Definition of Markov Chains
- Discrete-valued, discrete-time stochastic processes
	- A discrete-valued, discrete-time is a stochastic process
	- Mathematically, a discrete-valued, discrete-time stochastic process has the Markov property when P(Sn+1 = sn+1|Sn = sn) = P(Sn+1 = sn+1|Sn = sn, Sn−1 = sn−1, ...S1 = sn) for all si and for all n
	- That is, the probability distribution for the next state depends solely on the current state
	- We then say that Sn is a Markov process.
The analogous condition for continuous time is
P(S(tn+1) = sn+1|S(tn) = sn) =
P(S(tn+1) = sn+1|S(tn) = sn, S(tn−1) = sn−1, . . . S(t1) = s1) (14.2)
for all values si and any increasing sequence of times ti

## Random Walks


## References
- http://bactra.org/prob-notes/srl.pdf
